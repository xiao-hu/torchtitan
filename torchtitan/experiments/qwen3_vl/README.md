# Qwen3 VL MOE Implementation

Implementation of Qwen3 VLM (Vision-Language Model) with Mixture of Experts (MOE) to train "Qwen/Qwen3-VL-30B-A3B-Instruct"

## Overview

This implementation combines:
- **Qwen3 text-only model** with MOE support (from `torchtitan/models/qwen3/`)
- **SigLIP-2 vision encoder** (from `torchtitan/experiments/vlm/model/siglip2.py`)
- **DeepStack integration** - Visual features injected into early text decoder layers
- **Multi-dimensional RoPE** (MRoPE) for vision position encoding

## Architecture Components

### Key Features
- **Vision Encoder**: SigLIP-2 based encoder with patch merging
- **Text Decoder**: Qwen3 with MOE layers (sparse expert activation)
- **Projector**: Two-layer MLP mapping vision embeddings to text space
- **DeepStack**: Multi-layer visual feature integration
- **Special Tokens**: Image, video, vision_start, vision_end tokens
- **Position Encoding**: 3D RoPE for temporal, height, width dimensions

### MOE Configuration
- MOE layers inserted at specific decoder layers (`decoder_sparse_step`)
- Load balancing across experts during training
- Expert parallelism support for distributed training

## References

* **Qwen3 (text-only)**: `torchtitan/models/qwen3/`
* **VLM reference**: `torchtitan/experiments/vlm/model/` (Llama3Siglip2Transformer)
* **SigLIP-2 encoder**: `torchtitan/experiments/vlm/model/siglip2.py`
* **VLM datasets**: `torchtitan/experiments/vlm/datasets/`
* **HF implementation**: https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py
* **Qwen3-30B-A3B config**: `custom_task/qwen3_30b.toml`

## Implementation Tasks

### Phase 1: Project Setup & Structure ‚úÖ
- [x] Create project structure in `torchtitan/experiments/qwen3_vl/`
  - `model/` - Core model implementation directory
  - `datasets/` - Multimodal dataset utilities (TBD)
  - `tests/` - Unit and integration tests (TBD)
  - `train_configs/` - Training configuration TOML files (TBD)
- [x] Create all `__init__.py` files with proper exports

**File Status:**
- ‚úÖ Complete: `__init__.py`, `args.py`, `vision.py`, `README.md`
- ‚è≥ Scaffold: `model.py`, `state_dict_adapter.py`
- üìã Planned: `datasets/`, `tests/`, `train_configs/`

### Phase 2: Model Arguments (args.py) ‚úÖ
- [x] Create `Qwen3VLVisionArgs` dataclass with HF-aligned field names
  - All fields match `Qwen3VLVisionConfig` exactly: depth, hidden_size, intermediate_size, etc.
  - Enables trivial conversion to HF config (`**config.__dict__`)
  - Includes DeepStack parameters (deepstack_visual_indexes: [8, 16, 24])
  - No dependency on Siglip2ModelArgs (independent, purpose-built config)
- [x] Create `Qwen3VLModelArgs` class extending `Qwen3ModelArgs`
  - Inherits text decoder config (dim, n_layers, n_heads, moe_enabled, etc.)
  - Adds vision encoder config (vision_config: Qwen3VLVisionArgs)
  - Adds special token IDs (image_token_id, video_token_id, vision_start_token_id, vision_end_token_id)
  - Adds DeepStack integration parameter (deepstack_visual_indexes at model level)
- [x] Define `SpecialTokens` dataclass for VLM tokens
  - Structured token management (img, video, vision_start, vision_end, pad)
  - Factory method `from_tokenizer()` for extracting from HuggingFace tokenizer
  - Includes ignore_id for loss masking

### Phase 3: Vision Components ‚úÖ
**Implementation**: Import HF Qwen3VLVisionModel with thin wrapper
- [x] Create `Qwen3VLVisionArgs` with HF-aligned field names
  - All fields match `Qwen3VLVisionConfig` exactly (depth, hidden_size, etc.)
  - Eliminates conversion overhead (trivial `**config.__dict__` mapping)
- [x] Import HF `Qwen3VLVisionModel` as-is
  - Proven implementation (3D patching, spatial merging, DeepStack)
  - Zero maintenance burden (upstream bug fixes)
  - No code duplication (~60 lines wrapper vs ~300+ reimplementation)
- [x] Create thin `Qwen3VLVisionEncoder` wrapper
  - Config conversion: TorchTitan `Qwen3VLVisionArgs` ‚Üí HF `Qwen3VLVisionConfig`
  - Compatible forward signature for TorchTitan integration
  - No parallelism (vision encoder is small, sequential processing)
- [x] Update exports in `__init__.py`

**Benefits**:
- ‚úÖ Proven implementation (all features work: videos, temporal patches, merging, DeepStack)
- ‚úÖ Minimal code (60 lines vs 300-400)
- ‚úÖ Zero conversion overhead (field names aligned)
- ‚úÖ No external dependency at config level (only at runtime)

### Phase 4: Core Model Implementation (model.py) ‚úÖ COMPLETE
**Architecture**
```
Qwen3Model (base text decoder)
    ‚Üì extends
Qwen3VLTextModel (+ DeepStack injection)
    ‚Üë used by
Qwen3VLModel (vision + text integration)
```

**Implemented Components**:
- [x] `Qwen3VLTextModel` class (extends Qwen3Model)
  - [x] `_deepstack_process()` - Injects visual features at masked positions
  - [x] Extended `forward()` - Layer-wise DeepStack injection during decoding
  - [x] Backward compatible with optional vision parameters
  
- [x] `Qwen3VLModel` class (main VLM)
  - [x] Combines Qwen3VLVisionEncoder + Qwen3VLTextModel
  - [x] `get_image_features()` - Encodes images and returns embeddings + deepstack features
  - [x] `get_video_features()` - Processes videos (delegates to image features)
  - [x] Main `forward()` - Complete vision-text integration pipeline
  
- [x] Helper functions implemented
  - [x] `get_placeholder_mask()` - Vision token position identification and validation (~30 lines)
  - [x] `get_rope_index()` - 3D position IDs (T, H, W) for vision + 1D for text (~150 lines)
  
**Implementation Details**:
- [x] **Vision token handling**
  - Scatter vision embeddings into text token positions using `masked_scatter`
  - Support both image_token and video_token
  - Handle grid_thw (temporal, height, width) parameters
  
- [x] **DeepStack integration**
  - Vision encoder returns features from multiple layers
  - Inject into text decoder layers after forward pass
  - Maintain visual position masks throughout
  - Support merging image + video features when both present
  
- [x] **Forward pass**
  - Get text embeddings from input_ids
  - Encode vision inputs (images/videos) through vision encoder
  - Scatter vision embeddings into text embedding positions
  - Calculate 3D position IDs via get_rope_index
  - Process through Qwen3VLTextModel with DeepStack injection
  - Return model outputs (logits)

Load Model:
```python
import torchtitan
print(torchtitan.__version__)
from torchtitan.experiments.qwen3_vl import Qwen3VLModel, qwen3_vl_args
model_flavor: str = "30B-A3B"
model_args = qwen3_vl_args[model_flavor]
model = Qwen3VLModel(model_args)
print(f"Created Qwen3VLModel with {sum(p.numel() for p in model.parameters()):,} parameters")
```

### Phase 5: State Dict Adapter (state_dict_adapter.py) ‚úÖ COMPLETE
#### Implementation Tasks
- [x] Analyze HF checkpoint structure
  - Vision encoder keys: `model.visual.*` ‚Üí `visual.*`
  - Text model keys: `model.language_model.*` ‚Üí `language_model.*`
  - Reuses parent `Qwen3StateDictAdapter` for text + MOE handling
  
- [x] Implement key mappings
  - Vision encoder: Simple prefix transformation (`model.` add/remove)
  - Text decoder: Handle `model.language_model.*` correctly
  - Conversion logic: Strip/add `language_model.` infix as needed
  - MOE experts: Inherited from parent (standard per-expert format)
  
- [x] Handle special cases
  - Expert parameters: Inherited from `Qwen3StateDictAdapter`
  - Router weights: Handled by parent class
  - Weight tying: Correctly handled (lm_head ‚Üî embed_tokens)
  
- [x] Test checkpoint conversion
  - ‚úÖ 4/4 unit tests passing (updated with correct HF format)

#### Checkpoint Conversion Tool

A conversion script is provided to convert HuggingFace Qwen3-VL checkpoints to TorchTitan format:

**Location**: `custom_task/convert_hf_checkpoint.py`

**Features**:
- Loads HuggingFace Qwen3-VL checkpoint
- Converts to TorchTitan format using `Qwen3VLStateDictAdapter`
- Saves as PyTorch checkpoint
- Optional validation (converts back to verify correctness)
- Supports both dense and MOE models

**Usage**:

```bash
# Basic conversion
python custom_task/convert_hf_checkpoint.py \
    --hf-checkpoint /path/to/Qwen3-VL-30B-A3B-Instruct \
    --output-path /path/to/output/checkpoint.pt \
    --model-flavor 30B-A3B

# With validation (converts back to HF format to verify)
python custom_task/convert_hf_checkpoint.py \
    --hf-checkpoint /path/to/Qwen3-VL-30B-A3B-Instruct \
    --output-path /path/to/output/checkpoint.pt \
    --model-flavor 30B-A3B \
    --validate

# Test conversion without saving (useful for debugging)
python custom_task/convert_hf_checkpoint.py \
    --hf-checkpoint /path/to/Qwen3-VL-30B-A3B-Instruct \
    --output-path /tmp/test.pt \
    --model-flavor 30B-A3B \
    --skip-save
```

**Arguments**:
- `--hf-checkpoint` (required): Path to HuggingFace checkpoint directory
- `--output-path` (required): Path to save TorchTitan checkpoint (.pt file)
- `--model-flavor` (optional): Model configuration to use (`debugmodel` or `30B-A3B`, default: `30B-A3B`)
- `--validate` (optional): Validate conversion by converting back to HF format
- `--skip-save` (optional): Skip saving checkpoint (useful for testing)

**Output**:
- Saved checkpoint contains:
  - `model`: TorchTitan state dict (882 keys for 30B model)
  - `model_args`: Model configuration parameters

**Loading Converted Checkpoint**:

```python
import torch
from torchtitan.experiments.qwen3_vl import Qwen3VLModel

# Load checkpoint
checkpoint = torch.load('/path/to/checkpoint.pt')

# Get model args and create model
model_args = checkpoint['model_args']
model = Qwen3VLModel(model_args)

# Load weights
model.load_state_dict(checkpoint['model'])

# Ready for training!
```

**Tested With**:
- ‚úÖ Qwen3-VL-30B-A3B-Instruct (3.9B active params, 882 keys)
- ‚úÖ Dense models (67M parameters)
- ‚úÖ All vision + text keys converted correctly

### Phase 6: Dataset Integration
- [ ] Adapt multimodal datasets from `torchtitan/experiments/vlm/datasets/`
- [ ] Support inputs:
  - `pixel_values`: Image tensors
  - `pixel_values_videos`: Video tensors
  - `image_grid_thw`: Grid dimensions for images
  - `video_grid_thw`: Grid dimensions for videos
- [ ] Handle special tokens in sequences
  - `<|vision_start|>`, `<|image|>`, `<|video|>`, `<|vision_end|>`
- [ ] Implement proper padding and masking

### Phase 7: Training Configuration
- [ ] Create training config TOML
  - Based on `custom_task/qwen3_30b.toml`
  - Add vision preprocessing parameters
  - Configure MOE parallelism (expert_parallel_degree=2, expert_tensor_parallel_degree=4)
  - Add dataset configuration for multimodal data
  
- [ ] Model parameters
  ```toml
  [model]
  name = "qwen3_vl_moe"
  flavor = "30B-A3B"
  hf_assets_path = "/path/to/Qwen3-VL-30B-A3B-Instruct"
  
  [model.vision]
  patch_size = 14
  temporal_patch_size = 2
  spatial_merge_size = 2
  ```

### Phase 8: Testing & Validation
- [ ] Unit tests
  - Vision encoder forward pass
  - Projector forward pass
  - Token scattering logic
  - DeepStack integration
  - MRoPE position encoding
  
- [ ] Integration tests
  - Full model forward pass with dummy data
  - Checkpoint save/load
  - State dict conversion HF ‚Üî TorchTitan
  
- [ ] Numerical validation
  - Compare with HF implementation
  - Check gradient correctness
  - Verify MOE load balancing

### Phase 9: Optimization & Parallelism
- [ ] Configure tensor parallelism for vision encoder
- [ ] Configure expert parallelism for MOE layers
- [ ] Test parallelism configurations:
  - TP=4, EP=2, FSDP=auto
  - Expert TP=4 for shared experts
- [ ] Enable activation checkpointing (selective mode)
- [ ] Test torch.compile support (note: MOE may have limitations)

### Phase 10: Documentation & Examples
- [x] Update README.md with implementation plan
- [ ] Add usage examples
- [ ] Document model architecture details
- [ ] Add training script example
- [ ] Document special considerations (MRoPE, DeepStack, etc.)

## Technical Challenges

### 1. Multi-dimensional RoPE (MRoPE)
Qwen3 VL uses 3D position encoding for vision inputs:
```python
position_ids.shape = (3, batch_size, seq_len)  # [temporal, height, width]
```
- Must implement `get_rope_index()` for proper position tracking
- Handle different grids for images vs videos
- Maintain position deltas across generation steps

### 2. DeepStack Integration
Visual features from multiple encoder layers injected into decoder:
```python
deepstack_visual_indexes = [0, 1, 2]  # inject into first 3 decoder layers
```
- Extract features from vision encoder layers
- Maintain visual position masks
- Inject at correct decoder layer positions

### 3. Variable Length Vision Sequences
Images and videos have different sizes and temporal dimensions:
```python
grid_thw = [[1, 24, 24], [4, 12, 12]]  # [temporal, height, width]
```
- Proper masking for different sizes
- Position encoding for variable grids
- Efficient batching of mixed sizes

### 4. MOE + Vision Integration
Combining MOE text layers with vision features:
- Ensure vision features propagate through MOE layers
- Load balancing with multimodal inputs
- Proper expert parallelism configuration

### 5. State Dict Conversion
HF checkpoint structure differs from TorchTitan:
- Nested MOE expert weights require careful mapping
- Vision encoder key transformations
- Handle weight tying between embeddings and lm_head

## Project Structure

```
torchtitan/experiments/qwen3_vl/
‚îú‚îÄ‚îÄ README.md                 # This file
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ args.py              # Model configuration classes
‚îÇ   ‚îú‚îÄ‚îÄ model.py             # Main Qwen3VLMoeModel implementation
‚îÇ   ‚îî‚îÄ‚îÄ state_dict_adapter.py # HF ‚Üî TorchTitan checkpoint conversion
‚îú‚îÄ‚îÄ datasets/                # Multimodal dataset utilities (reuse from vlm/)
‚îú‚îÄ‚îÄ train_configs/           # Training configuration TOML files
‚îî‚îÄ‚îÄ tests/                   # Unit and integration tests
```

## Usage Example

```python
from torchtitan.experiments.qwen3_vl.model import Qwen3VLMoeModel
from torchtitan.experiments.qwen3_vl.model.args import Qwen3VLModelArgs

# Create model config
model_args = Qwen3VLModelArgs(
    dim=3072,
    n_layers=28,
    n_heads=24,
    vocab_size=151936,
    moe_enabled=True,
    num_experts=8,
    # ... other params
)

# Initialize model
model = Qwen3VLMoeModel(model_args)

# Forward pass
outputs = model(
    input_ids=input_ids,
    pixel_values=pixel_values,
    image_grid_thw=image_grid_thw,
    attention_mask=attention_mask,
)
```

## Dependencies

**Required:**
- PyTorch with CUDA support
- Existing Qwen3 model (`torchtitan/models/qwen3/`)
- SigLIP-2 vision encoder (`torchtitan/experiments/vlm/model/siglip2.py`)
- MOE infrastructure (`torchtitan/models/moe/`)
- VLM dataset utilities (`torchtitan/experiments/vlm/datasets/`)

**Optional:**
- HF `transformers` library (for checkpoint conversion)
- Multimodal evaluation datasets

## Estimated Timeline

- **Phase 1-2**: Setup and configuration (~3-4 hours)
- **Phase 3**: Vision components integration (~1 hour)
- **Phase 4**: Core model implementation (~8-12 hours)
- **Phase 5**: State dict adapter (~4-6 hours)
- **Phase 6-7**: Configuration and datasets (~3-4 hours)
- **Phase 8**: Testing and validation (~4-6 hours)
- **Phase 9-10**: Optimization and documentation (~3-4 hours)

**Total Estimated**: 25-40 hours

## Next Steps

1. ‚úÖ Complete analysis and task breakdown
2. Create project structure (Phase 1)
3. Implement args.py with configuration classes (Phase 2)
4. Begin core model implementation (Phase 4)
5. Implement state dict adapter (Phase 5)
6. Test with HF checkpoint and validate outputs (Phase 8)
