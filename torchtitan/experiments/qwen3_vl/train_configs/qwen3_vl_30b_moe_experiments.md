# Qwen3-VL 30B-A3B Training Experiments

**Model**: Qwen3-VL-30B-A3B-Instruct  
**Hardware**: 8x H200 (141GB)  
**Dataset**: VQAv2 validation

## Setup

```bash
pip install 'transformers>=4.37.0 tensordict'
CONFIG_FILE='./torchtitan/experiments/qwen3_vl/train_configs/qwen3_vl_30b_moe.toml' ./run_train.sh
```

---

## Experiments

### Exp 1: Baseline (No Packing)

```toml
local_batch_size = 1, seq_len = 8192, compile = false
```

**Results**: `memory: 63.54GiB (45%), tps: 25, mfu: 0.08%`

**Analysis**: Low GPU utilization due to short sequences (~400-800 tokens)

---

### Exp 2: Sample Packing

```toml
local_batch_size = 1, seq_len = 8192, compile = false
# Packing enabled with buffer_size=204
```

**Results**: `memory: 120.53GiB (86%), tps: 35, mfu: 0.14%`

**Analysis**:
- ‚úÖ +40% throughput (25‚Üí35 tps)
- ‚úÖ +75% MFU (0.08%‚Üí0.14%)
- ‚ö†Ô∏è Still very low MFU (should be 40-60%)
- ‚ö†Ô∏è CPU bottleneck (97-98% CPU utilization)

---

### Exp 3: Torch Compile

```toml
local_batch_size = 1, seq_len = 8192, compile = true
```

**Results**: `memory: 103.97GiB (74%), tps: 34, mfu: 0.14%`

**Analysis**:
- ‚úÖ Reduced memory usage
- ‚ùå No MFU improvement (step 2 is still compiling)
- ‚ùå CPU remains bottleneck (100% utilization)

**Note**: Compile warmup takes 20-40 steps due to variable packed shapes

---

## Bottleneck Analysis

**GPU Profile**: 30-38% utilization (idle 60-70%)  
**CPU Profile**: 97-98% utilization (MAXED OUT!)

**Time per Step**:
```
PIL decode + load:  60-100ms  ‚Üê CPU bottleneck
Sample packing:     50-80ms   ‚Üê CPU bottleneck  
Tokenization:       10-20ms   ‚Üê CPU bottleneck
GPU compute:        100-150ms
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Total:              220-350ms
GPU idle:           70-200ms  ‚Üê 60-70% wasted!
```

**Key Challenges**:
* Multi-process data workers are not supported in torchtitan yet: https://github.com/pytorch/torchtitan/issues/2073
* PIL causes **segmentation faults** with CUDA + multiprocessing
* The default qwen3-vl collator does not support variable image sizes across samples

### Exp 4: Text-Only Training Baseline

**Goal**: Verify that Qwen3-VL maintains comparable performance when trained on text-only datasets (no vision overhead).

**Configuration**:
```toml
[training]
local_batch_size = 4  # Increased for text-only (no vision memory overhead)
seq_len = 12288
dataset = "c4"  # Text-only dataset from DATASETS registry
```

**Results** (steady-state performance after warmup):
```
[rank0]:[titan] 2025-12-17 01:18:26,708 - root - INFO - step:  1  loss: 12.4153  grad_norm:  8.2782  memory: 87.87GiB(62.89%)  tps: 945  tflops: 47.71  mfu: 4.82%
[rank0]:[titan] 2025-12-17 01:18:26,708 - root - INFO - Synchronizing and adjusting timeout for all ProcessGroups to 0:01:40
[rank0]:[titan] 2025-12-17 01:18:34,942 - root - INFO - step:  2  loss: 12.3984  grad_norm:  8.0045  memory: 116.16GiB(83.14%)  tps: 5,970  tflops: 301.33  mfu: 30.47%
[rank0]:[titan] 2025-12-17 01:18:42,407 - root - INFO - step:  3  loss: 12.3513  grad_norm:  7.9040  memory: 116.16GiB(83.14%)  tps: 6,585  tflops: 332.35  mfu: 33.60%
[rank0]:[titan] 2025-12-17 01:18:49,878 - root - INFO - step:  4  loss: 12.3057  grad_norm:  8.1505  memory: 116.41GiB(83.32%)  tps: 6,580  tflops: 332.11  mfu: 33.58%
[rank0]:[titan] 2025-12-17 01:18:57,341 - root - INFO - step:  5  loss: 12.2364  grad_norm:  8.0841  memory: 116.41GiB(83.32%)  tps: 6,586  tflops: 332.44  mfu: 33.61%
[rank0]:[titan] 2025-12-17 01:19:04,815 - root - INFO - step:  6  loss: 12.1519  grad_norm:  8.1318  memory: 116.41GiB(83.32%)  tps: 6,577  tflops: 331.97  mfu: 33.57%
[rank0]:[titan] 2025-12-17 01:19:12,274 - root - INFO - step:  7  loss: 12.0584  grad_norm:  8.1898  memory: 116.41GiB(83.32%)  tps: 6,590  tflops: 332.64  mfu: 33.63%
[rank0]:[titan] 2025-12-17 01:19:19,772 - root - INFO - step:  8  loss: 11.8934  grad_norm:  9.1551  memory: 116.41GiB(83.32%)  tps: 6,556  tflops: 330.91  mfu: 33.46%
[rank0]:[titan] 2025-12-17 01:19:27,304 - root - INFO - step:  9  loss: 11.7361  grad_norm:  9.4663  memory: 116.51GiB(83.39%)  tps: 6,527  tflops: 329.44  mfu: 33.31%
[rank0]:[titan] 2025-12-17 01:19:34,836 - root - INFO - step: 10  loss: 11.5838  grad_norm:  9.6145  memory: 116.51GiB(83.39%)  tps: 6,526  tflops: 329.40  mfu: 33.31%
[rank0]:[titan] 2025-12-17 01:19:42,393 - root - INFO - step: 11  loss: 11.3803  grad_norm:  9.8774  memory: 116.51GiB(83.39%)  tps: 6,505  tflops: 328.34  mfu: 33.20%
[rank0]:[titan] 2025-12-17 01:19:49,999 - root - INFO - step: 12  loss: 11.2006  grad_norm:  9.5630  memory: 116.51GiB(83.39%)  tps: 6,462  tflops: 326.18  mfu: 32.98%
[rank0]:[titan] 2025-12-17 01:19:57,656 - root - INFO - step: 13  loss: 11.0067  grad_norm:  8.2690  memory: 116.51GiB(83.39%)  tps: 6,420  tflops: 324.06  mfu: 32.77%
[rank0]:[titan] 2025-12-17 01:20:05,335 - root - INFO - step: 14  loss: 10.8099  grad_norm:  8.3617  memory: 116.51GiB(83.39%)  tps: 6,401  tflops: 323.08  mfu: 32.67%
[rank0]:[titan] 2025-12-17 01:20:13,099 - root - INFO - step: 15  loss: 10.6878  grad_norm: 12.7063  memory: 116.51GiB(83.39%)  tps: 6,332  tflops: 319.60  mfu: 32.32%
[rank0]:[titan] 2025-12-17 01:20:20,849 - root - INFO - step: 16  loss: 10.5777  grad_norm: 12.1309  memory: 116.51GiB(83.39%)  tps: 6,343  tflops: 320.15  mfu: 32.37%
[rank0]:[titan] 2025-12-17 01:20:28,696 - root - INFO - step: 17  loss: 10.4500  grad_norm: 10.7103  memory: 116.51GiB(83.39%)  tps: 6,264  tflops: 316.19  mfu: 31.97%
[rank0]:[titan] 2025-12-17 01:20:36,556 - root - INFO - step: 18  loss: 10.3665  grad_norm:  8.8775  memory: 116.51GiB(83.39%)  tps: 6,255  tflops: 315.69  mfu: 31.92%
```

**Analysis**:
- ‚úÖ **Excellent performance**: ~6,500 tps, 330 tflops, **33% MFU**
- ‚úÖ **~185x throughput improvement** vs. VQA multimodal (35 tps ‚Üí 6,500 tps)
- ‚úÖ **~240x MFU improvement** vs. VQA multimodal (0.14% ‚Üí 33%)
- ‚úÖ Stable training with decreasing loss trajectory
- üí° **Key insight**: CPU bottleneck is eliminated with text-only data (no PIL decoding, simpler tokenization)

---

### Exp 5: Offline Data Preprocessing (Future Work)

**Goal**: Improve multimodal data loading efficiency through offline preprocessing.

**Approach**: Pre-process and cache vision data to reduce CPU bottleneck during training.

**Design**: See [offline data process](../datasets/offline_data_process.md) for detailed implementation.

**Expected Benefits**:
- Reduced CPU load during training
- Faster data loading pipeline
- Better GPU utilization for multimodal training

---

## Summary

| Experiment | Dataset | Config | TPS | MFU | Memory | Notes |
|------------|---------|--------|-----|-----|--------|-------|
| Exp 1 | VQAv2 | No packing, compile=false | 25 | 0.08% | 63.5GB (45%) | Baseline, low utilization |
| Exp 2 | VQAv2 | Packing enabled | 35 | 0.14% | 120.5GB (86%) | +40% throughput, CPU bottleneck |
| Exp 3 | VQAv2 | Compile=true | 34 | 0.14% | 104GB (74%) | Lower memory, still CPU bound |
| Exp 4 | C4 (text) | bs=4, seq=12288 | 6,500 | 33% | 116.5GB (83%) | **185x faster**, no CPU bottleneck |

**Key Findings**:
1. **Multimodal training is CPU-bound** due to PIL image decoding and data preprocessing
2. **Sample packing** provides moderate improvements (+40% throughput) but doesn't solve the CPU bottleneck
3. **Text-only training** achieves excellent performance (33% MFU), validating that the model and infrastructure are capable
4. **Root cause**: Image preprocessing dominates the training loop, leaving GPU idle 60-70% of the time

**Recommendations**:
- Implement offline data preprocessing (Exp 5) to cache processed images
- Consider alternative image loading libraries that support multiprocessing better
- Explore async data loading to overlap CPU preprocessing with GPU compute
